from io import StringIO
import json
import os
from typing import Dict, Literal, Optional, Any
import pandas as pd
import sqlite3

# LangChain imports
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage
from langchain_core.prompts import PromptTemplate
from langgraph.graph import StateGraph, END, START
from langgraph.types import Command
from langgraph.graph.message import MessagesState
from langgraph.checkpoint.memory import MemorySaver
from langchain.schema.runnable.config import RunnableConfig
from langgraph.prebuilt import create_react_agent

# Chainlit imports
import chainlit as cl
import chainlit.data as cl_data
from chainlit.data.sql_alchemy import SQLAlchemyDataLayer
from mcp import ClientSession
from mcp.types import CallToolResult, TextContent

# Local imports
from src.utils.utils import token_count, read_prompt, read_file_prompt, messages_token_count, load_chat_model, get_latest_human_message, reasoning_prompt
from src.db.db_query import generate_query, is_valid_query, query_summary

# Custom API
from fastapi import FastAPI, HTTPException, Request, Response, APIRouter
from starlette.responses import StreamingResponse
from chainlit.server import app
from starlette.routing import BaseRoute, Route

checkpointer=MemorySaver()

#-------------------------------
# System Constants
#-------------------------------
SYSTEM_PROMPT = read_file_prompt("./src/prompts/report_system_prompt.txt")

VALID_REPORT_CATEGORIES = {"code", "container", "aws", "kubernetes", "all"}

#-------------------------------
# Database Configuration
#-------------------------------
from src.db.db_setup import setup_database_connections

app_context = setup_database_connections()
#-------------------------------
# Model setup
#-------------------------------
model = load_chat_model()
final_model = load_chat_model().with_config(tags=["final_node"])

#-------------------------------
# Chainlit Authentication
#-------------------------------
@cl.header_auth_callback
def header_auth_callback(headers: Dict) -> Optional[cl.User]:
    """Authenticate users via header information"""
    return cl.User(identifier="admin", metadata={"role": "admin", "provider": "header"})

#-------------------------------
# State Definition
#-------------------------------
class AgentState(MessagesState):
    """State maintained throughout the agent's workflow"""
    intention: Optional[str] = None
    user_query: Optional[str] = None
    sql_query: Optional[str] = None
    query_results: Optional[str] = None
    category: Optional[str] = None
    result_text: Optional[str] = None
    top5: Optional[str] = None
    dataframe: Optional[str] = None
    tool_message: Optional[str] = None




#-------------------------------
# Helper Functions
#-------------------------------
def parse_report_command(input_string: str) -> str:
    """
    Parse a /report command and extract the category.
    Raises ValueError for invalid input.
    """
    command_prefix = "/report "
    
    if not input_string.startswith(command_prefix):
        raise ValueError("Input does not start with '/report'.")
        
    # Extract the argument after the prefix
    argument = input_string[len(command_prefix):].strip()
    
    if not argument:
        raise ValueError("No argument provided after '/report'.")
        
    if argument not in VALID_REPORT_CATEGORIES:
        raise ValueError(
            f"Invalid argument '{argument}'. Allowed arguments are "
            f"{', '.join(VALID_REPORT_CATEGORIES)}."
        )
        
    return argument

#-------------------------------
# Node Functions
#-------------------------------
async def classify_user_intent(state: AgentState):
    """
    Classify the user's query as either a report request or a regular question.
    """
    messages = state["messages"]
    query = get_latest_human_message(messages)
    print(f"\n\nUSER QUERY: {query} \n")
    
    try:
        # Try to parse as a report command
        category = parse_report_command(query)
        return Command(
            update={"category": category},
            goto="summary"
        )
    except ValueError:
        # Process as a regular question
        content = reasoning_prompt(
            "./src/prompts/intent_classification_prompt.txt", 
            question=query
        )
        intent_response = await model.ainvoke([HumanMessage(content=content)])
        
        try:
            res = json.loads(intent_response.content)
            score = res.get("Score", 0)
            
            if score > 30:
                return Command(
                    update={"intention": res, "user_query": query},
                    goto="querydb"
                )
            else:
                return Command(
                    update={"intention": res, "user_query": None},
                    goto="mcp_tool"
                )
        except json.JSONDecodeError:
            # Handle invalid JSON response
            print("Failed to parse intent classification response")
            return Command(
                update={"user_query": query},
                goto="mcp_tool"
            )
        
async def invoke_llm(state: AgentState):
    messages = state["messages"]
    response = await model.ainvoke(messages)
    # We return a list, because this will get added to the existing list
    return {"messages": [response]}

async def generate_summary_report(state: AgentState):
    """Generate a summary report based on the specified category"""
    print("--------------do_summary---------------")
    category = state["category"]

    # Query database for summary data
    summary_df, details_df = await query_summary(app_context.get_connection(), category)
    
    # Convert results to string format
    result = details_df.to_string(index=False)
    top5_result = details_df.to_string()
    summary = summary_df.to_string(index=False)

    # Format prompt for the model
    template = read_prompt("summary")
    prompt = PromptTemplate(
        template=template,
        input_variables=["category", "summary", "result"]
    )
    formatted_prompt = prompt.format(
        category=category, 
        summary=summary, 
        result=result
    )
    
    # Create messages for the model
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=formatted_prompt)
    ]

    # Log token usage
    tokens = token_count(formatted_prompt)
    print(f"Token used: {tokens}\n")

    # Get response from the model
    response = await final_model.ainvoke(messages)

    # Store results in state
    df_str = details_df.to_csv(index=False)
    return {
        "dataframe": df_str, 
        "result_text": result, 
        "top5": top5_result, 
        "messages": [response]
    }

async def generate_insights(state: AgentState):
    """Generate insights based on the top 5 results"""
    print("--------------do_insight---------------")
    result = state["top5"]

    # Format prompt for insights
    template = read_prompt("insight")
    prompt = PromptTemplate(
        template=template,
        input_variables=["result"]
    )
    formatted_prompt = prompt.format(result=result)
    
    # Create messages for the model
    messages = [
        SystemMessage(content=SYSTEM_PROMPT),
        HumanMessage(content=formatted_prompt)
    ]
    
    # Get response from the model
    response = await final_model.ainvoke(messages)

    return {"messages": [response]}

async def finalize_conclusion(state: AgentState):
    """Generate a conclusion based on the full results"""
    print("--------------do_conclude---------------")
    messages = state["messages"]
    result = state["result_text"]

    # Add conclusion prompt to messages
    template = read_prompt("conclude")
    messages.append(HumanMessage(content=template))
    
    # Log token usage
    total_tokens = messages_token_count(messages)
    print(f"total message tokens: {total_tokens}")
    
    # Get response from the model
    response = await final_model.ainvoke(messages)
    
    return {"messages": [HumanMessage(content=result), response]}

async def execute_db_query(state: AgentState) -> Command[Literal["reason"]]:
    """
    Execute a database query based on the user's question
    """
    messages = state["messages"]
    user_query = state["user_query"]
    
    # Determine category if available
    category = state.get("category", "ALL").upper() if state.get("category") else "ALL"

    try:
        # Generate a database query using the model
        generated_query = await generate_query(user_query, category, model)

        # Validate the generated query
        if not is_valid_query(generated_query, app_context.get_engine()):
            print("Generated query is invalid or potentially unsafe.\n\n")
            return Command(
                update={"user_query": user_query},
                goto="reason"
            )

        # Execute the validated query
        print("Executing query...\n\n")
        cursor = app_context.get_connection().cursor()
        cursor.execute(generated_query)
        records = cursor.fetchall()

        # Prepare query results
        if records:
            columns = [desc[0] for desc in cursor.description]
            results_str = "\n".join(str(dict(zip(columns, row))) for row in records)
        else:
            results_str = "No results returned."

        print("Query results prepared.\n\n")
        return Command(
            update={
                "user_query": user_query, 
                "sql_query": generated_query, 
                "query_results": results_str, 
                "messages": messages + [SystemMessage(content="Query executed successfully.")]
            },
            goto="mcp_tool"
        )

    except Exception as e:
        print(f"Error during query execution: {e}\n\n")
        return Command(
            update={"user_query": user_query},
            goto="mcp_tool"
        )

async def execute_mcp_tool(state: AgentState) -> Command[Literal["reason"]]:
    """
    Execute a tool call based on the user's question
    """
    tool_message = ""
    try:
        user_query = state.get("user_query", "")
        sql_query = state.get("sql_query", "")
        query_results = state.get("query_results", "")

        # Get tools from all MCP connections
        mcp_tools = cl.user_session.get("mcp_tools", {})
        all_tools = ""
        for conn_name, tools in mcp_tools.items():
            for tool in tools:
                all_tools += f"Tool Name: {tool['name']}\n"
                all_tools += f"Description: {tool['description']}\n"
                all_tools += f"Schema: {tool['input_schema']}\n"

        # Format the mcp prompt
        template = read_prompt("mcp")
        prompt = PromptTemplate(
            template=template,
            input_variables=["tools", "input", "sql_query", "scan_results"]
        )
        formatted_prompt = prompt.format(
            tools=all_tools, 
            input=user_query,
            sql_query=sql_query,
            scan_results=query_results
        )
        messages_history = [HumanMessage(content=formatted_prompt)]
        response = await model.ainvoke(messages_history)
        json_response = response.content.replace("```json", "").replace("```", "")

        try:
            tool_calls = json.loads(json_response)
            for tool_call in tool_calls:
                name = tool_call["name"]
                arguments = tool_call["arguments"]
                tool_result = await mcp_call_tool(name, arguments)
                tool_result_text = ""
                for content_item in tool_result.content:
                    if isinstance(content_item, TextContent):
                        tool_result_text += f"{(content_item.text)}\n"
                tool_message = tool_result_text
        except Exception as e:
            print(f"execute_mcp_tool: Error processing response: {e}, response: {json_response}")
        
    except Exception as e:
        print(f"execute_mcp_tool: Error during tool execution: {e}")
    
    return Command(
        update={"tool_message": tool_message},
        goto="reason"
    )

async def provide_explanation(state: AgentState):
    """
    Generate an explanation based on query results
    """
    try:
        user_query = state.get("user_query", "")
        sql_query = state.get("sql_query", "")
        query_results = state.get("query_results", "")
        tool_message = state.get("tool_message", "")

        # If the user query is missing, default to the latest human message
        if not user_query:
            user_query = get_latest_human_message(state["messages"])

        # Format the explanation prompt
        template = read_prompt("explanation")
        prompt = PromptTemplate(
            template=template,
            input_variables=["question", "sql_query", "scan_results", "tool_message"]
        )
        formatted_prompt = prompt.format(
            question=user_query, 
            sql_query=sql_query, 
            scan_results=query_results,
            tool_message=tool_message
        )
        
        # Limit prompt size to prevent token overflow
        if len(formatted_prompt) > 80000:
            formatted_prompt = formatted_prompt[:80000]

        response = await model.ainvoke([HumanMessage(content=formatted_prompt)])
        explanation_response = response.content

        # Clear state for next interaction
        return Command(
            update={
                "user_query": None,
                "sql_query": None,
                "query_results": None,
                "messages": state["messages"] + [explanation_response]
            }
        )

    except Exception as e:
        print(f"Error during explanation generation: {e}")
        return Command(
            update={
                "user_query": None,
                "sql_query": None,
                "query_results": None,
                "tool_message": None,
                "messages": state["messages"] + [
                    SystemMessage(content="An error occurred while generating the explanation. Please try again.")
                ]
            }
        )


#-------------------------------
# Graph node
#-------------------------------

builder = StateGraph(AgentState)

builder.add_node("intent", classify_user_intent)
builder.add_node("querydb", execute_db_query)
builder.add_node("summary", generate_summary_report)
builder.add_node("insight", generate_insights)
builder.add_node("mcp_tool", execute_mcp_tool)
builder.add_node("conclude", finalize_conclusion)
builder.add_node("reason", provide_explanation)
builder.add_node("report", invoke_llm)

# define the node which will display the resoning result on web
REASONING_NODE = ["reason", "report", "summary", "insight", "assessment", "remediation", "effort", "conclude"]

builder.add_edge(START, "intent")
builder.add_edge("summary", "insight")
builder.add_edge("insight", "conclude")
builder.add_edge("querydb", "reason")
builder.add_edge("querydb", "mcp_tool")
builder.add_edge("mcp_tool", "reason")
builder.add_edge("conclude", END)
builder.add_edge("reason", END)

graph = builder.compile(
checkpointer=checkpointer
)

#-------------------------------
# chainlit workflow
#-------------------------------

@cl.on_chat_start
async def on_chat_start():
    cl.user_session.set("chat_history",[])

@cl.on_message
async def on_message(msg: cl.Message):
    chat_history = cl.user_session.get("chat_history")
    chat_history.append({"role": "user", "content": msg.content})
    config = {"configurable": {"thread_id": msg.thread_id}}

    cb = cl.LangchainCallbackHandler()
    final_answer = cl.Message(content="")
    
    async for msg, metadata in graph.astream({"messages": [HumanMessage(content=msg.content)]}, stream_mode="messages", config=RunnableConfig(callbacks=[], **config)):
        if (
            msg.content
            and not isinstance(msg, HumanMessage)
            and not isinstance(msg, SystemMessage)
            and metadata["langgraph_node"] in REASONING_NODE
        ):
            await final_answer.stream_token(msg.content)

        if (
            "finish_reason" in msg.response_metadata
            and msg.response_metadata["finish_reason"] == "stop"
        ):
            await final_answer.stream_token("\n\n")

        # Hack print report by dataframe
        if (
            "finish_reason" in msg.response_metadata
            and msg.response_metadata["finish_reason"] == "stop"
            and metadata["langgraph_node"] in ["insight"]
        ):
            state = graph.get_state(config=config)
            df_str = state.values["dataframe"]
            df = pd.read_csv(StringIO(df_str))
            elements = [cl.Dataframe(data=df, display="inline", name="Dataframe")]
            await cl.Message(content="Report Table:", elements=elements).send()

    await final_answer.send()

@cl.set_starters
async def set_starters():
    return [
        cl.Starter(
            label="All scan executive summary report",
            message='/report all',
            icon="/public/ranking.png",
            ),
        cl.Starter(
            label="Kubernetes executive summary report",
            message='/report kubernetes',
            icon="/public/k8s.png",
            ),
        cl.Starter(
            label="AWS executive summary report",
            message="/report aws",
            icon="/public/aws.png",
            ),
        cl.Starter(
            label="Code executive summary report",
            message="/report code",
            icon="/public/code.png",
            ),
        cl.Starter(
            label="Container executive summary report",
            message="/report container",
            icon="/public/container.png",
            ),
    ]

@cl.on_chat_resume
async def on_chat_resume(thread):
    cl.user_session.set("chat_history", [])

    if thread.get("metadata") is not None:
        metadata = thread["metadata"]
        # check type of metadata of the thread, if it is a string, convert it to a dictionary
        if isinstance(metadata, str):
            metadata = json.loads(metadata)
        
        if metadata.get("chat_history") is not None:
            state_messages = []
            chat_history = metadata["chat_history"]
            for message in chat_history:
                cl.user_session.get("chat_history").append(message)
                if message["role"] == "user":
                    state_messages.append(HumanMessage(content=message["content"]))
                else:
                    state_messages.append(AIMessage(content=message["content"]))

            thread_id = thread["id"]
            config = {"configurable": {"thread_id": thread_id}}
            state = graph.get_state(config).values
            if "messages" not in state:
                state["messages"] = state_messages
                graph.update_state(config, state)

@cl.on_mcp_connect
async def on_mcp(connection, session: ClientSession):
    # List available tools
    result = await session.list_tools()

    # Process tool metadata
    tools = [{
        "name": t.name,
        "description": t.description,
        "input_schema": t.inputSchema,
    } for t in result.tools]
    
    # Store tools for later use
    mcp_tools = cl.user_session.get("mcp_tools", {})
    mcp_tools[connection.name] = tools
    cl.user_session.set("mcp_tools", mcp_tools)

@cl.on_mcp_disconnect
async def on_mcp_disconnect(name: str, session: ClientSession):
    mcp_tools = cl.user_session.get("mcp_tools", {})
    if name in mcp_tools:
        del mcp_tools[name]
        cl.user_session.set("mcp_tools", mcp_tools)

@cl.step(type="tool")
async def mcp_call_tool(tool_name: str, tool_input: Dict[str, Any]):
    print(f"mcp_call_tool: name = {tool_name}, input = {tool_input}")
    mcp_name = None
    mcp_tools = cl.user_session.get("mcp_tools", {})

    for conn_name, tools in mcp_tools.items():
        if any(tool["name"] == tool_name for tool in tools):
            mcp_name = conn_name
            break

    if not mcp_name:
        return {"error": f"Tool '{tool_name}' not found in any connected MCP server"}

    mcp_session, _ = cl.context.session.mcp_sessions.get(mcp_name)

    try:
        result = await mcp_session.call_tool(tool_name, tool_input)
        return result
    except Exception as e:
        return {"error": f"Error calling tool '{tool_name}': {str(e)}"}
    

cust_router = APIRouter()

@cust_router.get("/blob/{object_key}")
async def serve_blob_file(
    object_key: str
):
    if app_context.storage_client is None:
        raise HTTPException(status_code=500, detail="Storage client not initialized")
    file_data = await app_context.storage_client.download_file(object_key)
    
    return Response(content=file_data, media_type="application/octet-stream")

serve_route: list[BaseRoute] = [
    r for r in app.router.routes if isinstance(r, Route) and r.name == "serve"
]

for route in serve_route:
    app.router.routes.remove(route)

app.include_router(cust_router)
app.router.routes.extend(serve_route)
